# Basic course of Machine learning at MIPT.

[Открытый курс по ML от ФПМИ МФТИ](https://github.com/girafe-ai/ml-course)
Были изучены материалы курса, законспектированы лекции, выполнены лабораторные работы. Ниже приведено описание проделанной работы.

## My homeworks: 
|Тип|Название|Описание|
|---|--------|--------|
|lab|[ML Pipeline & SVM](https://github.com/Myashka/MIPT_Basic_ML/tree/main/Homeworks/Lab1_ML_pipeline_and_SVM)| Лабораторная работа состояла из 3-ёх частей.<br>1. В [первой части](https://github.com/Myashka/MIPT_Basic_ML/blob/main/Homeworks/Lab1_ML_pipeline_and_SVM/Lab1_part1_differentiation.ipynb) было выполнено матричное дифференцирование.<br>2. Во [второй части](https://github.com/Myashka/MIPT_Basic_ML/blob/main/Homeworks/Lab1_ML_pipeline_and_SVM/Lab1_part2_ml_pipeline.ipynb) были произведены эксперименты с данными: выполнена предобработка, визуализировано с PCA, построена модель логистической регрессии, оценён ROC-AUC score. Также были протестированы модели _RandomForest_ и _Bagging_ над решающими деревьями, сделана визуализация разделяющих гиперплоскостей от Дерева принятия решений. Оценен Learning curve.<br>3. В [третьей части](https://github.com/Myashka/MIPT_Basic_ML/blob/main/Homeworks/Lab1_ML_pipeline_and_SVM/Lab1_part3_SVM.ipynb) были произведены эксперименты с SVM классификатором над датасетом _make_moons_. Использованы различные ядра, построены визуализации, также использованы поиномиальные данные. Решен датасет _make_circules_.|
|lab|[DL](https://github.com/Myashka/MIPT_Basic_ML/tree/main/Homeworks/Lab2_DL)| Лабораторная работа состояла из 5-и частей 3 основные, 2 опциональные. Были выполнено 2-4 части. (первая часть была выполнена по ходу выполнения assigments).<br>1. Во [второй части](https://github.com/Myashka/MIPT_Basic_ML/blob/main/Homeworks/Lab2_DL/Lab2_DL_part2_overfitting.ipynb) я боролся с переобучением сети над датасетом FashionMNIST. Для этого была упрощена структура модели, добавлены Dropout слои.<br>2. В [третьей части](https://github.com/Myashka/MIPT_Basic_ML/blob/main/Homeworks/Lab2_DL/Lab2_DL_part3_poetry.ipynb) Было выполнена генерация стихов на основе Шейкспира. Для этого были предобработаны текстовые данные. Построена простая языковая Seq2Seq модель c LSTM. Произведены эксперименты с температурой SofrMax'a. То же самое было проделано с текстом Евгения Онегина.<br>3. В [четвертой части](https://github.com/Myashka/MIPT_Basic_ML/blob/main/Homeworks/Lab2_DL/Lab2_DL_parts_4_and_5_optional.ipynb) решалась задача HAR (human activity recognission). Для этого выполнена визуализация PCA c 2 компонентами, сформирован PyTorch датасет. Были произведены различные эксперименты по поводу каким именно образом и с какой инициализацией весов лучше работает LSTM для данной проблемы. Так была отмечена зависимость, что при использовании NLLLoss результаты лучше, чем с CrossEntropyLoss, вероятно из-за числовой стабильности. Accuracy = 0.9.|
|assignment|[kNN](https://github.com/Myashka/MIPT_Basic_ML/tree/main/Homeworks/assignment0_01_kNN)| Реализован класс _KNearestNeighbor_, классификатор на основе алгоритма ближайших соседей, измерена разница во времени работы алгоритма в зависимости от наличия циклов в имплементации. Произведено сравнение с _sklearn_ kNN. |
|assignment|[kNN](https://github.com/Myashka/MIPT_Basic_ML/tree/main/Homeworks/assignment0_01_kNN)| Реализован класс _KNearestNeighbor_, классификатор на основе алгоритма ближайших соседей, измерена разница во времени работы алгоритма в зависимости от наличия циклов в имплементации. Произведено сравнение с _sklearn_ kNN.|
|assignment|[Linear Regression](https://github.com/Myashka/MIPT_Basic_ML/tree/main/Homeworks/assignment0_02_Lin_reg)| Решены примеры на матричное дифференцирование. Имплементированы функции ошибок и производных в [loss_and_derivatives](https://github.com/Myashka/MIPT_Basic_ML/blob/main/Homeworks/assignment0_02_Lin_reg/loss_and_derivatives.py). Выполнен градиентный спуск c L2 регуляризацией, выполнено сравнение с _sklearn_.|
|assignment|[SVM](https://github.com/Myashka/MIPT_Basic_ML/tree/main/Homeworks/assignment0_03_SVM)| Имплементирован SVM метод с rbf ядром на основе интерфейса sklearn и градиентной оптимизацией PyTorch ([файл](https://github.com/Myashka/MIPT_Basic_ML/blob/main/Homeworks/assignment0_03_SVM/svm.py)) Протестирована работа и произведено сравнение с _sklearn_ SVC.|
|assignment|[Tree](https://github.com/Myashka/MIPT_Basic_ML/tree/main/Homeworks/assignment0_04_tree)| Имплементированы различные критерии информативности. Также структура дерева , создание сплита и выбор лучшего ([файл](https://github.com/Myashka/MIPT_Basic_ML/blob/main/Homeworks/assignment0_04_tree/tree.py)) Протестирована имплементация, выполнен _GridSearch_.|
|assignment|[NN from scratch](https://github.com/Myashka/MIPT_Basic_ML/tree/main/Homeworks/assignment0_05_NN_from_scratch)| Имплементированы различные слои для нейронной сети, такие как Linear, SoftMax, LogSoftMax, BatchNorm, DropOut. Функции активации: ReLU, LeakyReLu, ELU, SoftPlus. Функции потерь: MSE, NLL. Оптимизаторы: SGD Momentum, Adam. ([файл](https://github.com/Myashka/MIPT_Basic_ML/blob/main/Homeworks/assignment0_05_NN_from_scratch/modules.ipynb)) Реализованные модули Протестированы.|

## Seminars and practics:

Неделя| Семинар | Описание |
|-----|---------|----------|
|  01_1 |[Предобработка данных](https://github.com/Myashka/MIPT_Basic_ML/blob/main/week_01_2_data_preprocessing.ipynb) | Работы с данными в Pandas, работа с пропусками, преобразование нечисловых значений, добавление признаков.|
|  01_2 |[Naive Bias](https://github.com/Myashka/MIPT_Basic_ML/blob/main/week_01_2_naive_bayes.ipynb) | Naive Bias from Skratch implementation, сравнение с kNN, реализация ядерного сглаживания (KDE), сравнение результатов.|
|  02_1 |[Linear Regression, GD](https://github.com/Myashka/MIPT_Basic_ML/blob/main/week_02_1_linear_regression_SGD_pipeline_sklearn_class.ipynb) | Линейная регрессия, GD, SGD, несколько сюжетов про числа обусловленности, sklearn pipeline.|
|  02_2 |[Loss Functions, Regularization](https://github.com/Myashka/MIPT_Basic_ML/blob/main/week_02_2_regilazition_loss_functions.ipynb) | Вероятностный взгляд на MSE и регуляризацию, функции потерь.|
|  03_1 |[Logistic Regression with PyTorch](https://github.com/Myashka/MIPT_Basic_ML/blob/main/week_03_1_intro_to_pytorch_logistic_regression.ipynb) | Знакомство с PyTorch, AutoGrad, логистическая регрессия.|
|  04_1 |[SVM](https://github.com/Myashka/MIPT_Basic_ML/blob/main/week_04_1_SVM.ipynb) | Реализация SVM на основе sklearn и PyTorch. Различные опыты с Sklearn SVC.|
|  04_2 |[SVD](https://github.com/Myashka/MIPT_Basic_ML/blob/main/week_04_2_pictures_svd.ipynb) | Pictures compression with SVD.|
|  04_3 |[Bias Variance decomposition](https://github.com/Myashka/MIPT_Basic_ML/blob/main/week_04_3_BiasVariance.ipynb) | Bias Variance разложение, опыты с полиномами разных степеней.|
|  05_1 |[Decision trees](https://github.com/Myashka/MIPT_Basic_ML/blob/main/week_05_1_decision_trees.ipynb) | Деревья принятия решений, критерии информативности: Джини, энтропия Шеннона, ошибка классификации. Критерии для регрессии. Визуализации, pruning.
|  05_2 |[Ensembles](https://github.com/Myashka/MIPT_Basic_ML/blob/main/week_05_2_ensembles.ipynb) | Bootstrap, bagging, RandomForest. Эксперименты с RandomForest из sklearn, влияние гиперпараметров на обобщающую способность, feature importance.|
|  05_3 |[Cross_validation](https://github.com/Myashka/MIPT_Basic_ML/blob/main/week_05_3_cross_validation_riddle.ipynb) | Кросс-валидация, подбор гиперпараметров.|
|  06_1 |[Gradient Boosting](https://github.com/Myashka/MIPT_Basic_ML/blob/main/week_06_1_gradient_boosting.ipynb) | Градиентный бустинг, зависимость обучения от гиперпараметров.|
|  08_1 |[PyTorch, DataLoaders](https://github.com/Myashka/MIPT_Basic_ML/blob/main/week_08_1_PyTorch_and_Dataloaders.ipynb) | Обучение модели в PyTorch, работа с загрущиками данных.|
|  10_1 |[Embeddings, Seq2Seq](https://github.com/Myashka/MIPT_Basic_ML/blob/main/week_09_1_seq2seq_rnn.ipynb) | Embeddings, RNN, LSTM, генерация имен.|
|  11_1 |[CNN](https://github.com/Myashka/MIPT_Basic_ML/blob/main/week_10_cnn.ipynb) | Own class for CNN training, fine-tuning, Data Augmentation.|

